{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Product Entity Resolution with Kanoniv\n\nReconcile product catalogs from 4 heterogeneous retail feeds into a single canonical product catalog using the Kanoniv local SDK.\n\n| Source | Records | Key Identifiers | Challenge |\n|--------|---------|-----------------|----------|\n| ecommerce_catalog | ~2,600 | UPC (12-digit barcode), SKU | UPC format differs from GTIN |\n| wholesale_feed | ~2,600 | GTIN-13 (\"0\" + UPC), MPN | Different barcode format |\n| marketplace_listings | ~2,600 | ASIN only | No barcode or MPN |\n| retail_inventory | ~2,600 | manufacturer_code (= MPN) | No barcode |\n\n**10,000+ records** across 4 sources with ~3,000 ground-truth products.\n\n**What this covers:**\n- Full `ReconcileResult` API: clusters, golden records, decisions, telemetry, entity lookup\n- Iterative spec refinement: rules-based v1 -> Fellegi-Sunter v2\n- Three-layer evaluation: structural, stability, ground truth P/R/F1\n- Entity-level diffing with `ChangeLog`\n- Spec versioning with `diff()`\n- Persistence with `save()` / `load()`\n\n```bash\npip install kanoniv pandas\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(\"data\")\n",
    "\n",
    "ecom_df = pd.read_csv(DATA / \"ecommerce_catalog.csv\", dtype=str)\n",
    "whol_df = pd.read_csv(DATA / \"wholesale_feed.csv\", dtype=str)\n",
    "mkt_df = pd.read_csv(DATA / \"marketplace_listings.csv\", dtype=str)\n",
    "ret_df = pd.read_csv(DATA / \"retail_inventory.csv\", dtype=str)\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    {\"Source\": \"ecommerce\", \"Records\": len(ecom_df), \"Columns\": \", \".join(ecom_df.columns)},\n",
    "    {\"Source\": \"wholesale\", \"Records\": len(whol_df), \"Columns\": \", \".join(whol_df.columns)},\n",
    "    {\"Source\": \"marketplace\", \"Records\": len(mkt_df), \"Columns\": \", \".join(mkt_df.columns)},\n",
    "    {\"Source\": \"retail\", \"Records\": len(ret_df), \"Columns\": \", \".join(ret_df.columns)},\n",
    "])\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample rows from each source\n",
    "\n",
    "Notice: each source uses different column names, identifiers, and pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ecommerce (UPC barcode, product_name, brand, price_usd):\")\n",
    "display(ecom_df[[\"product_id\", \"product_name\", \"upc\", \"brand\", \"price_usd\"]].head(5))\n",
    "\n",
    "print(\"\\nWholesale (GTIN-13 barcode, MPN, item_name, manufacturer):\")\n",
    "display(whol_df[[\"item_id\", \"item_name\", \"gtin\", \"manufacturer\", \"mpn\", \"unit_cost\"]].head(5))\n",
    "\n",
    "print(\"\\nMarketplace (ASIN only - no barcode, no MPN):\")\n",
    "display(mkt_df[[\"listing_id\", \"title\", \"asin\", \"brand\", \"list_price\"]].head(5))\n",
    "\n",
    "print(\"\\nRetail (manufacturer_code = MPN, no barcode):\")\n",
    "display(ret_df[[\"inventory_id\", \"description\", \"manufacturer_code\", \"brand_name\", \"retail_price\"]].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key data challenges\n",
    "\n",
    "| Challenge | Example |\n",
    "|-----------|--------|\n",
    "| UPC (12-digit) vs GTIN-13 (\"0\" + UPC) | `030000000007` vs `0030000000007` |\n",
    "| MPN = manufacturer_code (different column names) | wholesale `mpn` = retail `manufacturer_code` |\n",
    "| Marketplace has no barcode or MPN | Must link via product name + brand only |\n",
    "| Name variations | `\"Samsung Galaxy Buds FE - New\"` vs `\"Samsung Galaxy Buds FE\"` |\n",
    "| Price differences | Wholesale cost < retail price < marketplace list price |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the UPC/GTIN format mismatch\n",
    "print(\"UPC vs GTIN format:\")\n",
    "print(f\"  Ecommerce UPC:   {ecom_df.iloc[0]['upc']}  (12 digits)\")\n",
    "\n",
    "# Find the matching GTIN\n",
    "gtin_lookup = \"0\" + ecom_df.iloc[0][\"upc\"]\n",
    "match = whol_df[whol_df[\"gtin\"] == gtin_lookup]\n",
    "if len(match) > 0:\n",
    "    print(f\"  Wholesale GTIN:  {match.iloc[0]['gtin']}  (13 digits = '0' + UPC)\")\n",
    "    print(f\"\\n  Same product:\")\n",
    "    print(f\"    ecommerce: {ecom_df.iloc[0]['product_name']}\")\n",
    "    print(f\"    wholesale: {match.iloc[0]['item_name']}\")\n",
    "\n",
    "# Count known overlaps\n",
    "ecom_upcs = set(ecom_df[\"upc\"])\n",
    "whol_gtins = set(whol_df[\"gtin\"])\n",
    "barcode_matches = sum(1 for upc in ecom_upcs if \"0\" + upc in whol_gtins)\n",
    "\n",
    "whol_mpns = set(whol_df[\"mpn\"])\n",
    "ret_mfgs = set(ret_df[\"manufacturer_code\"])\n",
    "mpn_matches = len(whol_mpns & ret_mfgs)\n",
    "\n",
    "ecom_names = set(ecom_df[\"product_name\"].str.lower().str.strip())\n",
    "mkt_names = set(mkt_df[\"title\"].str.lower().str.strip())\n",
    "name_matches = len(ecom_names & mkt_names)\n",
    "\n",
    "print(f\"\\nKnown linkages:\")\n",
    "print(f\"  Barcode (UPC->GTIN): {barcode_matches} ecommerce<->wholesale\")\n",
    "print(f\"  MPN exact:           {mpn_matches} wholesale<->retail\")\n",
    "print(f\"  Name exact:          {name_matches} ecommerce<->marketplace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Spec v1 - Rules-Based Matching\n",
    "\n",
    "First attempt: `weighted_sum` scoring with explicit rules.\n",
    "\n",
    "The spec maps each source's columns to canonical attribute names, defines blocking keys, matching rules, and survivorship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from kanoniv import Spec, Source, validate, plan, diff, reconcile, ReconcileResult\nimport textwrap\n\nSPEC_V1 = textwrap.dedent(\"\"\"\\\n    api_version: kanoniv/v2\n    identity_version: product_v1.0\n\n    entity:\n      name: product\n\n    sources:\n      - name: ecommerce\n        system: csv\n        table: ecommerce_catalog\n        id: product_id\n        attributes:\n          barcode: upc\n          product_name: product_name\n          brand: brand\n          price: price_usd\n          sku: sku\n\n      - name: wholesale\n        system: csv\n        table: wholesale_feed\n        id: item_id\n        attributes:\n          barcode: gtin\n          product_name: item_name\n          brand: manufacturer\n          mpn: mpn\n          price: unit_cost\n\n      - name: marketplace\n        system: csv\n        table: marketplace_listings\n        id: listing_id\n        attributes:\n          product_name: title\n          brand: brand\n          price: list_price\n\n      - name: retail\n        system: csv\n        table: retail_inventory\n        id: inventory_id\n        attributes:\n          product_name: description\n          brand: brand_name\n          mpn: manufacturer_code\n          price: retail_price\n\n    blocking:\n      strategy: composite\n      keys:\n        - [brand]\n\n    rules:\n      - name: barcode_exact\n        type: exact\n        field: barcode\n        weight: 1.0\n\n      - name: mpn_exact\n        type: exact\n        field: mpn\n        weight: 0.9\n\n      - name: name_fuzzy\n        type: similarity\n        field: product_name\n        algorithm: jaro_winkler\n        threshold: 0.97\n        weight: 0.6\n\n      - name: brand_exact\n        type: exact\n        field: brand\n        weight: 0.3\n\n    decision:\n      scoring: weighted_sum\n      thresholds:\n        match: 0.99\n        review: 0.7\n\n    survivorship:\n      default: most_complete\n      overrides:\n        - field: product_name\n          strategy: source_priority\n          priority: [ecommerce, marketplace, wholesale, retail]\n        - field: price\n          strategy: aggregate\n          function: min\n        - field: barcode\n          strategy: source_priority\n          priority: [ecommerce, wholesale]\n\"\"\")\n\nspec_v1 = Spec.from_string(SPEC_V1)\nprint(f\"Entity:  {spec_v1.entity}\")\nprint(f\"Version: {spec_v1.version}\")\nprint(f\"Sources: {len(spec_v1.sources)}\")\nprint(f\"Rules:   {len(spec_v1.rules)}\")\nfor r in spec_v1.rules:\n    print(f\"  {r['name']:18s}  type={r['type']:12s}  weight={r.get('weight')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate and plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr = validate(spec_v1)\n",
    "print(f\"Valid: {vr.valid}\")\n",
    "if not vr.valid:\n",
    "    for e in vr.errors:\n",
    "        print(f\"  ERROR: {e}\")\n",
    "\n",
    "plan_v1 = plan(spec_v1)\n",
    "print(f\"\\n{plan_v1.summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. First Reconciliation\n",
    "\n",
    "Load sources, run the engine, explore every part of `ReconcileResult`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [\n",
    "    Source.from_csv(\"ecommerce\",   str(DATA / \"ecommerce_catalog.csv\"),   primary_key=\"product_id\"),\n",
    "    Source.from_csv(\"wholesale\",   str(DATA / \"wholesale_feed.csv\"),      primary_key=\"item_id\"),\n",
    "    Source.from_csv(\"marketplace\", str(DATA / \"marketplace_listings.csv\"), primary_key=\"listing_id\"),\n",
    "    Source.from_csv(\"retail\",      str(DATA / \"retail_inventory.csv\"),     primary_key=\"inventory_id\"),\n",
    "]\n",
    "\n",
    "import time\n",
    "t0 = time.perf_counter()\n",
    "result_v1 = reconcile(sources, spec_v1)\n",
    "elapsed = time.perf_counter() - t0\n",
    "\n",
    "total_records = sum(len(c) for c in result_v1.clusters)\n",
    "print(f\"Input records: {total_records:,}\")\n",
    "print(f\"Clusters:      {result_v1.cluster_count:,}\")\n",
    "print(f\"Merge rate:    {result_v1.merge_rate:.1%}\")\n",
    "print(f\"Runtime:       {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Clusters\n",
    "\n",
    "Each cluster is a list of internal UUIDs representing records the engine decided belong to the same product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi = [c for c in result_v1.clusters if len(c) > 1]\n",
    "single = [c for c in result_v1.clusters if len(c) == 1]\n",
    "\n",
    "print(f\"Multi-record clusters: {len(multi)}\")\n",
    "print(f\"Singletons:            {len(single)}\")\n",
    "\n",
    "# Cluster size distribution\n",
    "from collections import Counter\n",
    "sizes = Counter(len(c) for c in result_v1.clusters)\n",
    "print(f\"\\nCluster size distribution:\")\n",
    "for size in sorted(sizes):\n",
    "    print(f\"  size {size:>2d}: {sizes[size]:>4d} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Golden records\n",
    "\n",
    "`result.to_pandas()` returns the merged canonical records with survivorship applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_golden = result_v1.to_pandas()\n",
    "print(f\"Golden records: {len(df_golden)} rows, {len(df_golden.columns)} columns\")\n",
    "\n",
    "# Show key fields\n",
    "display_cols = [\"kanoniv_id\", \"product_name\", \"brand\", \"price\", \"barcode\", \"mpn\", \"member_count\"]\n",
    "existing = [c for c in display_cols if c in df_golden.columns]\n",
    "df_golden[existing].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Match decisions\n",
    "\n",
    "Every candidate pair evaluated by the engine produces a decision: `merge`, `review`, or `nomerge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = result_v1.decisions\n",
    "print(f\"Total decisions: {len(decisions):,}\")\n",
    "\n",
    "# Count by type\n",
    "from collections import Counter\n",
    "dec_counts = Counter(d.get(\"decision\") for d in decisions)\n",
    "for dtype, count in sorted(dec_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {dtype:10s}: {count:,}\")\n",
    "\n",
    "# Show a merge decision\n",
    "merges = [d for d in decisions if d.get(\"decision\") == \"merge\"]\n",
    "if merges:\n",
    "    print(f\"\\nExample merge decision:\")\n",
    "    for k, v in merges[0].items():\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Telemetry\n",
    "\n",
    "Engine performance metrics: pairs evaluated, blocking groups, per-rule hit rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tel = result_v1.telemetry\n",
    "print(f\"Pairs evaluated:  {tel.get('pairs_evaluated', 0):,}\")\n",
    "print(f\"Blocking groups:  {tel.get('blocking_groups', 0):,}\")\n",
    "\n",
    "# Per-rule stats\n",
    "rule_tel = tel.get(\"rule_telemetry\", [])\n",
    "if rule_tel:\n",
    "    rule_df = pd.DataFrame([\n",
    "        {\n",
    "            \"Rule\": rt.get(\"rule_name\"),\n",
    "            \"Evaluated\": rt.get(\"evaluated\", 0),\n",
    "            \"Matched\": rt.get(\"matched\", 0),\n",
    "            \"Match Rate\": f\"{rt.get('matched', 0) / rt.get('evaluated', 1):.1%}\",\n",
    "            \"Avg Score\": f\"{rt.get('avg_score', 0):.3f}\",\n",
    "        }\n",
    "        for rt in rule_tel\n",
    "    ])\n",
    "    display(rule_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e. Entity lookup\n",
    "\n",
    "Reverse index mapping every source record to its canonical `kanoniv_id`. Use this to join back to operational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_df = result_v1.entity_lookup  # property, not method\n",
    "print(f\"Entity lookup: {len(lookup_df)} rows\")\n",
    "display(lookup_df.head(10))\n",
    "\n",
    "# Show all records for one product\n",
    "sample_kid = lookup_df.iloc[0][\"kanoniv_id\"]\n",
    "members = lookup_df[lookup_df[\"kanoniv_id\"] == sample_kid]\n",
    "print(f\"\\nAll source records for {sample_kid[:24]}...:\")\n",
    "display(members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Evaluation - Layers 1 + 2\n",
    "\n",
    "`result.evaluate()` returns structural and stability metrics without needing labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_v1 = result_v1.evaluate()\n",
    "print(metrics_v1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key metrics as a table\n",
    "pd.DataFrame([\n",
    "    {\"Metric\": \"Total Records\", \"Value\": metrics_v1.total_records},\n",
    "    {\"Metric\": \"Total Clusters\", \"Value\": metrics_v1.total_clusters},\n",
    "    {\"Metric\": \"Merge Rate\", \"Value\": f\"{metrics_v1.merge_rate:.1%}\"},\n",
    "    {\"Metric\": \"Singletons\", \"Value\": f\"{metrics_v1.singletons} ({metrics_v1.singletons_pct:.1%})\"},\n",
    "    {\"Metric\": \"Largest Cluster\", \"Value\": metrics_v1.largest_cluster},\n",
    "    {\"Metric\": \"Pairs Evaluated\", \"Value\": f\"{metrics_v1.pairs_evaluated:,}\"},\n",
    "    {\"Metric\": \"Blocking Groups\", \"Value\": metrics_v1.blocking_groups},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Ground Truth Evaluation (P/R/F1)\n",
    "\n",
    "Build ground truth from known deterministic linkages:\n",
    "1. **Barcode**: ecommerce UPC -> wholesale GTIN (prepend \"0\")\n",
    "2. **MPN**: wholesale `mpn` = retail `manufacturer_code`\n",
    "3. **Name**: ecommerce `product_name` = marketplace `title` (exact, case-insensitive)\n",
    "\n",
    "Then compute pairwise precision, recall, and F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Union-find for building ground truth clusters\n",
    "parent = {}\n",
    "\n",
    "def find(x):\n",
    "    if x not in parent:\n",
    "        parent[x] = x\n",
    "    while parent[x] != x:\n",
    "        parent[x] = parent[parent[x]]\n",
    "        x = parent[x]\n",
    "    return x\n",
    "\n",
    "def union(a, b):\n",
    "    ra, rb = find(a), find(b)\n",
    "    if ra != rb:\n",
    "        parent[ra] = rb\n",
    "\n",
    "# Link 1: ecommerce UPC <-> wholesale GTIN\n",
    "ecom_by_upc = dict(zip(ecom_df[\"upc\"], ecom_df[\"product_id\"]))\n",
    "whol_by_gtin = dict(zip(whol_df[\"gtin\"], whol_df[\"item_id\"]))\n",
    "barcode_links = 0\n",
    "for upc, eid in ecom_by_upc.items():\n",
    "    gtin = \"0\" + upc\n",
    "    if gtin in whol_by_gtin:\n",
    "        union((\"ecommerce\", eid), (\"wholesale\", whol_by_gtin[gtin]))\n",
    "        barcode_links += 1\n",
    "\n",
    "# Link 2: wholesale MPN <-> retail manufacturer_code\n",
    "whol_by_mpn = dict(zip(whol_df[\"mpn\"], whol_df[\"item_id\"]))\n",
    "ret_by_mfg = dict(zip(ret_df[\"manufacturer_code\"], ret_df[\"inventory_id\"]))\n",
    "mpn_links = 0\n",
    "for mpn, wid in whol_by_mpn.items():\n",
    "    if mpn in ret_by_mfg:\n",
    "        union((\"wholesale\", wid), (\"retail\", ret_by_mfg[mpn]))\n",
    "        mpn_links += 1\n",
    "\n",
    "# Link 3: ecommerce name <-> marketplace title\n",
    "ecom_by_name = {n.lower().strip(): pid for n, pid in zip(ecom_df[\"product_name\"], ecom_df[\"product_id\"])}\n",
    "mkt_by_name = {t.lower().strip(): lid for t, lid in zip(mkt_df[\"title\"], mkt_df[\"listing_id\"])}\n",
    "name_links = 0\n",
    "for name, eid in ecom_by_name.items():\n",
    "    if name in mkt_by_name:\n",
    "        union((\"ecommerce\", eid), (\"marketplace\", mkt_by_name[name]))\n",
    "        name_links += 1\n",
    "\n",
    "# Collect clusters\n",
    "gt_clusters = defaultdict(list)\n",
    "for record in parent:\n",
    "    gt_clusters[find(record)].append(record)\n",
    "\n",
    "# Format for evaluate(): {entity_id: [(source, id), ...]}\n",
    "ground_truth = {\n",
    "    f\"gt_{i}\": members\n",
    "    for i, members in enumerate(gt_clusters.values())\n",
    "    if len(members) >= 2\n",
    "}\n",
    "\n",
    "print(f\"Ground truth:\")\n",
    "print(f\"  Barcode links (ecom<->wholesale):  {barcode_links}\")\n",
    "print(f\"  MPN links (wholesale<->retail):     {mpn_links}\")\n",
    "print(f\"  Name links (ecom<->marketplace):    {name_links}\")\n",
    "print(f\"  Ground truth clusters:              {len(ground_truth)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_v1_gt = result_v1.evaluate(ground_truth=ground_truth)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"Metric\": \"Precision\", \"Value\": f\"{metrics_v1_gt.precision:.4f}\"},\n",
    "    {\"Metric\": \"Recall\", \"Value\": f\"{metrics_v1_gt.recall:.4f}\"},\n",
    "    {\"Metric\": \"F1\", \"Value\": f\"{metrics_v1_gt.f1:.4f}\"},\n",
    "    {\"Metric\": \"True Positives\", \"Value\": metrics_v1_gt.true_positives},\n",
    "    {\"Metric\": \"False Positives\", \"Value\": metrics_v1_gt.false_positives},\n",
    "    {\"Metric\": \"False Negatives\", \"Value\": metrics_v1_gt.false_negatives},\n",
    "    {\"Metric\": \"Predicted Pairs\", \"Value\": metrics_v1_gt.predicted_pairs},\n",
    "    {\"Metric\": \"Ground Truth Pairs\", \"Value\": metrics_v1_gt.ground_truth_pairs},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Spec v2 - Fellegi-Sunter Probabilistic Matching\n",
    "\n",
    "Switch from `weighted_sum` to `fellegi_sunter`. Key differences:\n",
    "\n",
    "| | Weighted Sum | Fellegi-Sunter |\n",
    "|---|---|---|\n",
    "| Score formula | `sum(weight * match) / sum(weight)` | `sum(weight * log2(m/u))` |\n",
    "| Missing fields | Penalizes (contributes 0 to numerator, weight to denominator) | Null-aware (contributes 0) |\n",
    "| Training | None | EM estimates m/u from data |\n",
    "| Score scale | 0.0 - 1.0 | Log-likelihood (unbounded) |\n",
    "\n",
    "FS handles partial evidence gracefully - when barcode and MPN are both missing (marketplace<->retail), name + brand agreement alone can drive a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SPEC_V2 = textwrap.dedent(\"\"\"\\\n    api_version: kanoniv/v2\n    identity_version: product_v2.0\n\n    entity:\n      name: product\n\n    sources:\n      - name: ecommerce\n        system: csv\n        table: ecommerce_catalog\n        id: product_id\n        attributes:\n          barcode: upc\n          product_name: product_name\n          brand: brand\n          price: price_usd\n          sku: sku\n\n      - name: wholesale\n        system: csv\n        table: wholesale_feed\n        id: item_id\n        attributes:\n          barcode: gtin\n          product_name: item_name\n          brand: manufacturer\n          mpn: mpn\n          price: unit_cost\n\n      - name: marketplace\n        system: csv\n        table: marketplace_listings\n        id: listing_id\n        attributes:\n          product_name: title\n          brand: brand\n          price: list_price\n\n      - name: retail\n        system: csv\n        table: retail_inventory\n        id: inventory_id\n        attributes:\n          product_name: description\n          brand: brand_name\n          mpn: manufacturer_code\n          price: retail_price\n\n    blocking:\n      strategy: composite\n      keys:\n        - [brand]\n\n    decision:\n      scoring:\n        strategy: fellegi_sunter\n        fields:\n          - name: barcode\n            comparator: exact\n            weight: 2.0\n            m_probability: 0.95\n            u_probability: 0.001\n            normalizer: generic\n          - name: mpn\n            comparator: exact\n            weight: 1.5\n            m_probability: 0.90\n            u_probability: 0.005\n            normalizer: generic\n          - name: product_name\n            comparator: jaro_winkler\n            weight: 1.0\n            m_probability: 0.85\n            u_probability: 0.03\n            normalizer: generic\n          - name: brand\n            comparator: exact\n            weight: 0.8\n            m_probability: 0.95\n            u_probability: 0.05\n            normalizer: generic\n        thresholds:\n          match: 5.6\n          possible: 2.0\n          non_match: -4.0\n      thresholds:\n        match: 0.9\n        review: 0.7\n\n    survivorship:\n      default: most_complete\n      overrides:\n        - field: product_name\n          strategy: source_priority\n          priority: [ecommerce, marketplace, wholesale, retail]\n        - field: price\n          strategy: aggregate\n          function: min\n        - field: barcode\n          strategy: source_priority\n          priority: [ecommerce, wholesale]\n\"\"\")\n\nspec_v2 = Spec.from_string(SPEC_V2)\nvalidate(spec_v2).raise_on_error()\nprint(\"Spec v2 valid.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spec diff (v1 -> v2)\n",
    "\n",
    "`diff()` shows exactly what changed between two spec versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = diff(spec_v1, spec_v2)\n",
    "\n",
    "print(f\"Has changes:        {d.has_changes}\")\n",
    "print(f\"Version changed:    {d.version_changed}\")\n",
    "print(f\"Rules removed:      {d.rules_removed}\")\n",
    "print(f\"Scoring changed:    {d.scoring_changed}\")\n",
    "print(f\"Thresholds changed: {d.thresholds_changed}\")\n",
    "print(f\"Blocking changed:   {d.blocking_changed}\")\n",
    "print(f\"\\nSummary: {d.summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. V2 Reconciliation + ChangeLog\n",
    "\n",
    "Run v2, then use `changes_since()` to see exactly which entities merged, split, grew, or were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter()\n",
    "result_v2 = reconcile(sources, spec_v2)\n",
    "elapsed = time.perf_counter() - t0\n",
    "\n",
    "print(f\"V2 clusters:   {result_v2.cluster_count:,}\")\n",
    "print(f\"V2 merge rate: {result_v2.merge_rate:.1%}\")\n",
    "print(f\"V2 runtime:    {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changelog = result_v2.changes_since(result_v1)\n",
    "\n",
    "print(f\"Summary: {changelog.summary}\")\n",
    "print(f\"\\nCreated:   {len(changelog.created):>3d}  (new entities)\")\n",
    "print(f\"Grown:     {len(changelog.grown):>3d}  (gained records)\")\n",
    "print(f\"Merged:    {len(changelog.merged):>3d}  (v1 entities combined)\")\n",
    "print(f\"Split:     {len(changelog.split):>3d}  (v1 entity broke apart)\")\n",
    "print(f\"Removed:   {len(changelog.removed):>3d}  (lost all records)\")\n",
    "print(f\"Unchanged: {changelog.unchanged_count:>3d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a merged entity\n",
    "if changelog.merged:\n",
    "    m = changelog.merged[0]\n",
    "    print(f\"Example merged entity:\")\n",
    "    print(f\"  kanoniv_id:    {m.kanoniv_id}\")\n",
    "    print(f\"  Members:       {len(m.source_records)}\")\n",
    "    print(f\"  Previous IDs:  {len(m.previous_kanoniv_ids)} entities combined\")\n",
    "    for src, rid in m.source_records[:5]:\n",
    "        print(f\"    {src:14s} {rid}\")\n",
    "\n",
    "# ChangeLog as DataFrame\n",
    "cl_df = changelog.to_pandas()\n",
    "print(f\"\\nChangeLog DataFrame: {len(cl_df)} rows\")\n",
    "display(cl_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. V1 vs V2 Comparison\n",
    "\n",
    "Side-by-side evaluation with ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_v2_gt = result_v2.evaluate(ground_truth=ground_truth)\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\"Metric\": \"Clusters\", \"V1 (weighted_sum)\": result_v1.cluster_count, \"V2 (fellegi_sunter)\": result_v2.cluster_count},\n",
    "    {\"Metric\": \"Merge rate\", \"V1 (weighted_sum)\": f\"{result_v1.merge_rate:.1%}\", \"V2 (fellegi_sunter)\": f\"{result_v2.merge_rate:.1%}\"},\n",
    "    {\"Metric\": \"Precision\", \"V1 (weighted_sum)\": f\"{metrics_v1_gt.precision:.4f}\", \"V2 (fellegi_sunter)\": f\"{metrics_v2_gt.precision:.4f}\"},\n",
    "    {\"Metric\": \"Recall\", \"V1 (weighted_sum)\": f\"{metrics_v1_gt.recall:.4f}\", \"V2 (fellegi_sunter)\": f\"{metrics_v2_gt.recall:.4f}\"},\n",
    "    {\"Metric\": \"F1\", \"V1 (weighted_sum)\": f\"{metrics_v1_gt.f1:.4f}\", \"V2 (fellegi_sunter)\": f\"{metrics_v2_gt.f1:.4f}\"},\n",
    "    {\"Metric\": \"True Positives\", \"V1 (weighted_sum)\": metrics_v1_gt.true_positives, \"V2 (fellegi_sunter)\": metrics_v2_gt.true_positives},\n",
    "    {\"Metric\": \"False Positives\", \"V1 (weighted_sum)\": metrics_v1_gt.false_positives, \"V2 (fellegi_sunter)\": metrics_v2_gt.false_positives},\n",
    "    {\"Metric\": \"False Negatives\", \"V1 (weighted_sum)\": metrics_v1_gt.false_negatives, \"V2 (fellegi_sunter)\": metrics_v2_gt.false_negatives},\n",
    "])\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load\n",
    "\n",
    "Persist results for later comparison or incremental reconciliation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_v2.save(\"product_v2.knv\")\n",
    "\n",
    "restored = ReconcileResult.load(\"product_v2.knv\")\n",
    "print(f\"Saved and restored: {restored.cluster_count} clusters\")\n",
    "assert restored.cluster_count == result_v2.cluster_count\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "os.remove(\"product_v2.knv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Full V2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics_v2_gt.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary\n\n### What we did\n\n1. Loaded 4 product catalogs (10,000+ records) with different schemas and identifiers\n2. Wrote a rules-based spec (v1) with `weighted_sum` scoring\n3. Ran reconciliation and inspected every part of `ReconcileResult`\n4. Evaluated with structural metrics (no labels needed) and ground truth P/R/F1\n5. Upgraded to Fellegi-Sunter (v2) with null-aware log-likelihood scoring\n6. Used `diff()` to compare spec versions and `changes_since()` to compare results\n7. Demonstrated `save()` / `load()` for persistence\n\n### API coverage\n\n| API | Section |\n|-----|--------|\n| `Source.from_csv()` | 3 |\n| `Spec.from_string()` | 2, 6 |\n| `validate()` | 2, 6 |\n| `plan()` | 2 |\n| `diff()` | 6 |\n| `reconcile()` | 3, 7 |\n| `result.clusters` | 3a |\n| `result.to_pandas()` | 3b |\n| `result.decisions` | 3c |\n| `result.telemetry` | 3d |\n| `result.entity_lookup` | 3e |\n| `result.cluster_count` / `merge_rate` | 3 |\n| `result.evaluate()` | 4 |\n| `result.evaluate(ground_truth=)` | 5, 8 |\n| `result.changes_since()` | 7 |\n| `result.save()` / `ReconcileResult.load()` | 8 |\n| `EvaluateResult.summary()` | 4, 9 |\n| `ChangeLog` | 7 |\n| `DiffResult` | 6 |\n\n### Next steps\n\n- **Active learning**: Label uncertain pairs via the [Cloud API](https://kanoniv.com/docs/sdks/cloud/) feedback endpoint and retrain FS with supervised EM\n- **Blocking experiments**: Try `[[product_name]]` or `[[brand, category]]` keys and measure the recall/candidate-pairs tradeoff\n- **Incremental reconciliation**: Pass `previous=result_v2` to `reconcile()` when new data arrives\n- **Cloud deployment**: Use `kanoniv.Client` for persistent identity graph, real-time resolve, and incremental export"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}